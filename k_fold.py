import numpy as np
from dataset import ClassifierDataset
from eval import Evaluator
from classification import DecisionTreeClassifier
import matplotlib.pyplot as plt

class k_fold_validator:

    '''
    Constructor initializes a dataset object and k (after checking that its value is acceptable)
    '''
    def __init__(self, k, path_to_dataset):
        self.dataset = ClassifierDataset()
        self.dataset.initFromFile(path_to_dataset)
        assert k == 0 or self.dataset.totalInstances % k == 0, \
            "k must be more than 0 and a factor of the length of dataset: ({})" \
            .format(int(self.dataset.totalInstances))
        self.k = k

    '''
    Function to split the (indices of the) dataset into 10 folds.
    Generate two arrays, one containing test indices and the other containing
    corresponding test_indices
    For example, if the dataset contains 3 rows (0,1,2) and k=3,

    train_indices = [[0,1], [0,2], [1,2]]
    test_indices =  [[2], [1], [0]]
    '''
    def split_dataset(self):
        n_rows = self.dataset.totalInstances # Find number of instances
        rows_per_split = int(n_rows/self.k)  # Size of each fold

        # Create a shuffled arra of integers from 0 to n_rows-1
        indices = np.random.permutation(n_rows)

        # Setup 2D array to hold test indices
        self.test_indices = np.ndarray((self.k, rows_per_split), dtype=np.int32)

        # Save consecutivechunks of the shuffled indices as test indices
        for i in range(self.k):
            base_index = i*rows_per_split
            self.test_indices[i] = indices[base_index:base_index+rows_per_split]

        # Setup 2D array to hold train indices
        self.train_indices = np.ndarray((self.k, n_rows - rows_per_split), dtype=np.int32)

        # Add all indices to each element of the test set, except for those already used in the train set
        for j in range(self.k):
            self.train_indices[j] = [i for i in indices if i not in self.test_indices[j]]

    '''
    Train and test k different models on each train/test split.
    Return average accuracy and standard deviation.
    '''
    def perform_validation(self):
        self.split_dataset()
        self.accuracy_scores = np.zeros(self.k, dtype=np.float64)
        all_class_labels = np.sort(np.unique(self.dataset.labels))

        for i in range(self.k):
            #load train and test splits
            train_attribs = self.dataset.attrib[self.train_indices[i]]
            train_labels = self.dataset.labels[self.train_indices[i]]
            test_attribs = self.dataset.attrib[self.test_indices[i]]
            test_labels = self.dataset.labels[self.test_indices[i]]

            tree = DecisionTreeClassifier()
            tree.train(train_attribs, train_labels)
            predictions = tree.predict(test_attribs)

            output_path = "model_{}.pickle".format(int(i))
            tree.writeToFile(output_path)

            e = Evaluator()
            c_matrix = e.confusion_matrix(predictions, test_labels, all_class_labels)

            self.accuracy_scores[i] = e.accuracy(c_matrix)

        avg_accuracy = np.average(self.accuracy_scores)
        std_dev = np.std(self.accuracy_scores)

        return avg_accuracy, std_dev
    '''
    Find and evaluate the performance of the most accurate model
    generated by the perform_validation function
    '''
    def test_best_model(self, test_path):
        max_index = np.argmax(self.accuracy_scores)
        model_path = "model_" + str(max_index) + ".pickle"
        tree = DecisionTreeClassifier()
        tree.readFromFile(model_path)

        test_dataset = ClassifierDataset()
        test_dataset.initFromFile(test_path)

        predictions = tree.predict(test_dataset.attrib)
        evaluator = Evaluator()
        c_matrix = evaluator.confusion_matrix(predictions, test_dataset.labels)
        labels = np.sort(np.unique(test_dataset.labels))

        #return stats
        plot_confusion_matrix(c_matrix, labels, "Most Accurate Model")
        precision, macro_p = evaluator.precision(c_matrix)
        recall, macro_r = evaluator.recall(c_matrix)
        f1, macro_f1 = evaluator.f1_score(c_matrix)

        p = np.append(precision, macro_p)
        r = np.append(recall, macro_r)
        f1 = np.append(f1, macro_f1)
        performance_matrix = np.vstack((p, np.vstack((r, f1))))

        x_labels = np.append(labels, "Macro avg")
        plot_other_stats(performance_matrix, x_labels, "Most Accurate Model")
        return



'''
Plot confusion matrices
'''
def plot_confusion_matrix(cm,target_names,title,cmap=None,normalize=False):
    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    fig, ax = plt.subplots()
    cmap = plt.get_cmap('Blues')
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title("Confusion matrix: " + title, pad = 10)
    plt.colorbar()
    ax.set_xticks(np.arange(len(target_names)))
    ax.set_yticks(np.arange(len(target_names)))
    ax.set_xticklabels(target_names)
    ax.set_yticklabels(target_names)
    ax.tick_params(top=True, bottom=False,
                   labeltop=True, labelbottom=False)
    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            c = cm[j,i]
            if normalize:
                ax.text(i, j, "{:0.4f}".format(c),
                         horizontalalignment="center",
                         color="white" if cm[i, j] > thresh else "black")
            else:
                ax.text(i, j, "{:,}".format(c),
                         horizontalalignment="center",
                         color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('Ground Truth')
    ax.set_xlabel('Predicted labels')
    ax.xaxis.set_label_position('top')

    text = "Accuracy={:0.4f}; Misclassification={:0.4f}".format(accuracy, misclass)
    plt.text(0.3,6.0, text)
    plt.tight_layout()
    plt.savefig("conf_" + title + '.pdf', bbox_inches='tight')
    return

'''
Plot F1, recall and precision scores
'''
def plot_other_stats(perf_mat, x_labels, plt_title):
    fig, ax = plt.subplots()
    cmap = plt.get_cmap('Blues')
    im = ax.imshow(perf_mat, interpolation='nearest', cmap=cmap)
    plt.title("Performance metrics: " + plt_title, y=1.15)
    plt.colorbar(im, fraction = 0.02, pad = 0.04)
    ax.set_xticks(np.arange(7))
    ax.set_yticks(np.arange(3))
    #x_labels = ["A", "C", "E", "G", "O", "Q", "Macro avg"]
    y_labels = ["Precision", "Recall", "F-1"]
    ax.set_xticklabels(x_labels)
    ax.set_yticklabels(y_labels)
    ax.tick_params(top=True, bottom=False,
                   labeltop=True, labelbottom=False)

    for i in range(perf_mat.shape[1]):
        for j in range(perf_mat.shape[0]):
            c = perf_mat[j, i]
            ax.text(i, j, "{:0.4f}".format(c), va='center', ha='center')

    plt.savefig("perf_" + plt_title + '.pdf', bbox_inches='tight')
    return

'''
Helper function for majority voting
'''
def calc_mode(a, axis=0):
    scores = np.unique(np.ravel(a))
    testshape = list(a.shape)
    testshape[axis] = 1
    oldmostfreq = np.zeros(testshape)
    oldcounts = np.zeros(testshape)
    for score in scores:
        template = (a == score)
        counts = np.expand_dims(np.sum(template, axis),axis)
        mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)
        oldcounts = np.maximum(counts, oldcounts)
        oldmostfreq = mostfrequent
    return mostfrequent, oldcounts

'''
Make predictions using 10 different models and return the modes of all predictions
'''
def ensemble(path_to_data):
    test_dataset = ClassifierDataset()
    test_dataset.initFromFile(path_to_data)
    all_predictions = np.ndarray((10,len(test_dataset.labels)), dtype=np.object)

    for i in range(10):
        model_path = "model_" + str(i) + ".pickle"
        tree = DecisionTreeClassifier()
        tree.readFromFile(model_path)
        predictions = tree.predict(test_dataset.attrib)
        all_predictions[i]  = predictions

    mode = calc_mode(all_predictions)[0]
    mode = mode.flatten()
    evaluator = Evaluator()
    c_matrix = evaluator.confusion_matrix(mode, test_dataset.labels)
    labels = np.sort(np.unique(test_dataset.labels))
    #return stats
    plot_confusion_matrix(c_matrix, labels, "Ensemble")

    precision, macro_p = evaluator.precision(c_matrix)
    recall, macro_r = evaluator.recall(c_matrix)
    f1, macro_f1 = evaluator.f1_score(c_matrix)

    p = np.append(precision, macro_p)
    r = np.append(recall, macro_r)
    f1 = np.append(f1, macro_f1)

    performance_matrix = np.vstack((p, np.vstack((r, f1))))
    x_labels = np.append(labels, "Macro avg")
    plot_other_stats(performance_matrix, x_labels, "Ensemble")
    return




path_to_data = "./data/train_full.txt"
test_path = "./data/test.txt"

kf = k_fold_validator(10, path_to_data)
avg_accuracy, std_dev = kf.perform_validation()
print(avg_accuracy, std_dev)
kf.test_best_model(test_path)
ensemble(test_path)
